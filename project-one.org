#+Title: Project One
#+Author: Simon Schnake
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{amssymb}
#+Inlineimages
#+OPTIONS: toc:nil

* Setup                                                            :noexport:
Here are the needed packages. Also to config matplotlib for latex export
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  mpl.rcParams['text.usetex'] = True
  mpl.rcParams['text.latex.preamble'] = [r'\usepackage{amsmath}']
  mpl.rcParams['mathtext.fontset'] = 'stix'
  mpl.rcParams['font.family'] = 'STIXGeneral'

  %matplotlib inline
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[163]:
:END:

* Loading Data                                                     :noexport:
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import h5py
data = h5py.File("../data/pion.h5")
X_test = data['test']['X']
Y_test = data['test']['Y']
X_train = data['train']['X']
Y_train = data['train']['Y']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
:END:

* Experiment Setup
Comparing different models
-
- Results charged particles in each scintialor cell
- true energy

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import numpy as np
  tracks = np.sum(X_test[:10000], axis=1)
  energies = np.transpose(Y_test[:10000])[0]


  plt.plot(tracks, energies, 'r.', alpha=0.03)
  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum n$')
  plt.savefig('e-vs-sum_n.pdf')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[169]:
[[file:./obipy-resources/7YTGkx.png]]
:END:

[[./e-vs-sum_n.pdf]]

* Linear Fit
A traditional way of calibrating the neural net would be to sum over all scintilator cells and perfom
a linear fit to the energy.
\[E = c_1\sum_i n_i + c_2\]
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export

#+END_SRC

* First Network
We are starting with a dense neural network and training
* Are we learning the shape?
** TODO divide the incoming cells by the total sum
* Data Augmentation
One of the biggest issues with the setup is that it
leads to fast overfitting models. To compensate this, there are
different ways. One way is to artificially increase the dataset by
data augmentation. Data Augmentation means generating new data by transformation of the given data.
TODO <- BESSER ERKLÃ„REN.
An easy to understand example is shown in the image of the cat.  Examples
of data augmentation are every form of flipping, rotations or
cutting. In the perspective of a physicist, data augmentation could be
interpreted as a form of making the data invariant under symmetry
transformations. This is of course only a subset of the possible ways
of data augmentation, but it should be enough for our
application. Calorimeter events should be invariant under rotations
perpendicular to the direction of the incoming particle. Our image of
the physical process is processed in rectangular structures, which
makes only rotations around $\frac{\pi}{2}$ valid transformations.  To
reduce computation and data costs we randomly apply this
transformations on incoming data into our network. So we do not
tranform data we are not training on and we are not storing additional
transformed datasets.

** TODO Einbauen

* Result
* Problem
* Analytical Solution
* Adverserial Solution
* Correction
