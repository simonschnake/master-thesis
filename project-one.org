#+Title: Project One
#+Author: Simon Schnake
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{amssymb}
#+OPTIONS: toc:nil

* Setup                                                            :noexport:
Here are the needed packages. Also to config matplotlib for latex export
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  mpl.rcParams['text.usetex'] = True
  mpl.rcParams['text.latex.preamble'] = [r'\usepackage{amsmath}']
  mpl.rcParams['mathtext.fontset'] = 'stix'
  mpl.rcParams['font.family'] = 'STIXGeneral'
  mpl.rcParams['axes.labelsize'] = 15
  %matplotlib inline
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[134]:
:END:

* Loading Data                                                     :noexport:
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import h5py
data = h5py.File("../data/electron.h5", 'r')
X_test = data['test']['X']
Y_test = data['test']['Y']
X_train = data['train']['X']
Y_train = data['train']['Y']
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[135]:
:END:

* Experiment Setup
Comparing different models
- Results charged particles in each scintialor cell
- true energy

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import numpy as np
  sum_n = np.sum(X_test[:10000], axis=1)
  energies = np.transpose(Y_test[:10000])[0]

  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')
  plt.savefig('e-vs-sum_n.pdf')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[136]:
[[file:./obipy-resources/thezxa.png]]
:END:

#+CAPTION: The graph shows the relation between the energies of the incoming particle $E_{\text{true}}$ in GeV and the absolute number of charged particles in all scintilator cells. 10000 points from the data are plotted.
#+NAME: e-vs-sum_n
[[./e-vs-sum_n.pdf]]

* Linear Fit
A traditional way of calibrating the neural net would be to sum over all scintilator cells as shown in [cite:figure1]
a linear fit to the energy.
\[E = c_0\sum_i n_i + c_1\]
This is done with the method of least squares.
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from scipy.optimize import leastsq
  # function to fit
  fitfunc = lambda c , x: x*c[0]+c[1]
  errfunc = lambda c , x, y: (y - fitfunc(c, x))
  out = leastsq(errfunc, [20., 0.0], args=(sum_n, energies), full_output=1)

  c_fit = out[0]
  covar = out[1]

  n = np.arange(0, 255, 0.5)
  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.plot(n, fitfunc(c_fit, n), 'k-')     # Fit

  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')

  plt.text(1, 9, r'$c_0 =$ '+ '%.2f' % c_fit[0], ha='left')
  plt.text(1, 8.3, r'$c_1 =$ '+ '%.2f' % c_fit[1], ha='left')
  plt.savefig('e-vs-sum_n_fit.pdf')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[137]:
[[file:./obipy-resources/7Y2dEm.png]]
:END:

#+CAPTION: The graph is the same as in \cite{e-vs-sum_n}. The black straight is the result of the fit described above.
#+NAME: e-vs-sum_n_fit
[[./e-vs-sum_n_fit.pdf]]

\[
   \text{PDF} = \frac{\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}}{\int^b_a \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \dd x} = \frac{\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}}{1/2(\text{erf}(\frac{\mu-a}{\sqrt{2}\sigma}) - \text{erf}(\frac{\mu-b}{\sqrt{2}\sigma}))} 
\]
\[
\text{likelihood} = \prod_{i} \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))}
\]

\[
\text{cost} = - \sum_{i} \ln( \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))})
\]

\[
\text{loss} = - \ln( \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))})
\]
\[
\text{loss} = - \ln(e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}})-\ln(\sqrt{2\pi \sigma_i^2}/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})))
\]
\[
\text{loss} = \frac{(E_i-\mu_i)^2}{2 \sigma_i^2}-\ln(\sqrt{2\pi \sigma_i^2}/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})))
\]

\[
\text{loss} = \frac{(E_i-\mu_i)^2}{2 \sigma_i^2}-\ln(\sqrt{\frac{\pi}{2}} \sigma_i\left(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})\right))
\]

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from scipy.special import erf
  from scipy.stats import norm
  from scipy import optimize

  def likelihood(par):
      epsilon = 0.000001
      mu = sum_n*par[0]+par[1]
      sigma = np.sqrt(np.abs(mu)) # 2/par[0] 
      elements = norm.pdf(energies, mu, sigma)
      a = np.divide(mu-lower_border, np.sqrt(2)*sigma+epsilon)
      b = np.divide(mu-upper_border, np.sqrt(2)*sigma+epsilon)
      norms = np.abs(erf(a)- erf(b))
      return -np.sum(np.log(np.divide(elements, norms + epsilon)+epsilon))

  lower_border = 0  
  upper_border = 10
  out = optimize.minimize(likelihood, np.array(c_fit), method='L-BFGS-B')
  
  c_like = out['x']

  n = np.arange(0, 235, 0.5)
  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.plot(n, fitfunc(c_like, n), 'k-')     # Fit

  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')

  plt.text(1, 9, r'$c_0 =$ '+ '%.2f' % c_like[0], ha='left')
  plt.text(1, 8.3, r'$c_1 =$ '+ '%.2f' % c_like[1], ha='left')

  plt.savefig('e-vs-sum_n_lik.pdf')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[138]:
[[file:./obipy-resources/bltOvL.png]]
:END:

#+CAPTION: The graph is the same as in \cite{e-vs-sum_n}. The black straight is the result of the likelihood fit described above.
#+NAME: e-vs-sum_n_lik
[[./e-vs-sum_n_lik.pdf]] 

* First Network

We are starting with a fully connected neural network. It is build out of 4 layers with 64 neurons each. The activation function of each layer is ReLu.
Our output layer is one neuron with a linear activation. We train the network with the rmsprop optimizer and our loss function is the mean squared error between
the true energy values and our predicted energy values. We train the network for 10 epochs with a batch size of 128.

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from keras.models import Sequential
  from keras.layers import Dense

  model = Sequential()
  model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(1, activation='linear'))

  model.compile(optimizer='rmsprop',
                loss='mse')

  loss_hist = np.empty(0)
  val_loss_hist = np.empty(0)
  epoch_arr = np.empty(0)
  hist = h5py.File('history.h5', 'w')
  hist.create_dataset('loss_hist', data=loss_hist)
  hist.create_dataset('val_loss_hist', data=val_loss_hist)
  hist.close()

  model.save('model.h5')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[139]:
:END:

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import pickle
history = pickle.load(open('history.p', 'rb'))
epochs = range(len(history['loss']))
fig, ax = plt.subplots()
ax.spines['top'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
plt.tick_params(axis='both', which='both', bottom=False, top=False,
                labelbottom=True, left=True, right=False, labelleft=True)
ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))
plt.plot(epochs, history['loss'], 'k-')
plt.plot(epochs, history['val_loss'], '-', color='#1f77b4')
plt.text(float(epochs[-1])+0.5, history['loss'][-1]-0.006, 'training loss', ha='left', size=15)
plt.text(float(epochs[-1])+0.5, history['val_loss'][-1]-0.006, 'validation loss', ha='left', size=15, color='#1f77b4')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.savefig('loss.pdf', bbox_inches = 'tight')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[140]:
[[file:./obipy-resources/eGnlxB.png]]
:END:


#+CAPTION: The loss and the validation loss are shown
#+NAME: Loss
[[./loss.pdf]]


** Server Stuff :noexport:
Here we copy stuff to the server and back
#+BEGIN_SRC shell
  scp model.h5 max-wgs:/home/schnakes/neural-net/
  scp max-wgs:/home/schnakes/neural-net/history.p .
  ssh max-wgs 'rm /home/schnakes/neural-net/history.p'
#+END_SRC

* Are we learning the shape?
** TODO divide the incoming cells by the total sum
* Data Augmentation
One of the biggest issues with the setup is that it
leads to fast overfitting models. To compensate this, there are
different ways. One way is to artificially increase the dataset by
data augmentation. Data Augmentation means generating new data by transformation of the given data.
TODO <- BESSER ERKLÃ„REN.
An easy to understand example is shown in the image of the cat.  Examples
of data augmentation are every form of flipping, rotations or
cutting. In the perspective of a physicist, data augmentation could be
interpreted as a form of making the data invariant under symmetry
transformations. This is of course only a subset of the possible ways
of data augmentation, but it should be enough for our
application. Calorimeter events should be invariant under rotations
perpendicular to the direction of the incoming particle. Our image of
the physical process is processed in rectangular structures, which
makes only rotations around $\frac{\pi}{2}$ valid transformations.  To
reduce computation and data costs we randomly apply this
transformations on incoming data into our network. So we do not
tranform data we are not training on and we are not storing additional
transformed datasets.

** TODO Einbauen

* Result
* Problem
* Analytical Solution
* Adverserial Solution
* Correction
