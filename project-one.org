#+Title: Project One
#+Author: Simon Schnake
#+LATEX_HEADER: \usepackage{physics}
#+LATEX_HEADER: \usepackage{amssymb}
#+OPTIONS: toc:nil

* Planning                                                         :noexport:
** TODO Write likelihood-loss chapter
** TODO Implement hyperparameter optimzing in likelihood-loss.py
** TODO Write adverserial training chapter
** TODO Implement Data Augmentation

* Setup                                                            :noexport:
Here are the needed packages. Also to config matplotlib for latex export
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  mpl.rcParams['text.usetex'] = True
  mpl.rcParams['text.latex.preamble'] = [r'\usepackage{amsmath}']
  mpl.rcParams['mathtext.fontset'] = 'stix'
  mpl.rcParams['font.family'] = 'STIXGeneral'
  mpl.rcParams['font.size'] = 15
  mpl.rcParams['axes.labelsize'] = 15

  %matplotlib inline
#+END_SRC

* Loading Data                                                     :noexport:
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import h5py
data = h5py.File("../data/electron.h5", 'r')
X_test = data['test']['X']
Y_test = data['test']['Y']
X_train = data['train']['X']
Y_train = data['train']['Y']
#+END_SRC

* Experiment Setup
Comparing different models
- Results charged particles in each scintialor cell
- true energy

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import numpy as np
  sum_n = np.sum(X_test[:10000], axis=1)
  energies = np.transpose(Y_test[:10000])[0]

  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')
  plt.savefig('e-vs-sum_n.pdf')
#+END_SRC

#+CAPTION: The graph shows the relation between the energies of the incoming particle $E_{\text{true}}$ in GeV and the absolute number of charged particles in all scintillator cells. 10000 points from the data are plotted.
#+NAME: e-vs-sum_n
[[./e-vs-sum_n.pdf]]

* Linear Fit
A traditional way of calibrating the neural net would be to sum over all scintillator cells as shown in [cite:figure1]
a linear fit to the energy.
\[E = c_0\sum_i n_i + c_1\]
This is done with the method of least squares.
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from scipy.optimize import leastsq
  # function to fit
  fitfunc = lambda c , x: x*c[0]+c[1]
  errfunc = lambda c , x, y: (y - fitfunc(c, x))
  out = leastsq(errfunc, [0.1, 0.0], args=(sum_n, energies), full_output=1)

  c_fit = out[0]
  covar = out[1]

  n = np.arange(0, 255, 0.5)
  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.plot(n, fitfunc(c_fit, n), 'k-')     # Fit

  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')

  plt.text(1, 9, r'$c_0 =$ '+ '%.2f' % c_fit[0], ha='left')
  plt.text(1, 8.3, r'$c_1 =$ '+ '%.2f' % c_fit[1], ha='left')
  plt.savefig('e-vs-sum_n_fit.pdf')
#+END_SRC

#+CAPTION: The graph is the same as in \cite{e-vs-sum_n}. The black straight is the result of the fit described above.
#+NAME: e-vs-sum_n_fit
[[./e-vs-sum_n_fit.pdf]]



\[
   \text{PDF} = \frac{\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}}{\int^b_a \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \dd x} = \frac{\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}}{1/2(\text{erf}(\frac{\mu-a}{\sqrt{2}\sigma}) - \text{erf}(\frac{\mu-b}{\sqrt{2}\sigma}))} 
\]
\[
\text{likelihood} = \prod_{i} \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))}
\]

\[
\text{cost} = - \sum_{i} \ln( \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))})
\]

\[
\text{loss} = - \ln( \frac{\frac{1}{\sqrt{2\pi \sigma_i^2}} e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}}}{1/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i}))})
\]
\[
\text{loss} = - \ln(e^{-\frac{(E_i-\mu_i)^2}{2 \sigma_i^2}})-\ln(\sqrt{2\pi \sigma_i^2}/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})))
\]
\[
\text{loss} = \frac{(E_i-\mu_i)^2}{2 \sigma_i^2}-\ln(\sqrt{2\pi \sigma_i^2}/2(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})))
\]

\[
\text{loss} = \frac{(E_i-\mu_i)^2}{2 \sigma_i^2}+\ln(\sqrt{\frac{\pi}{2}} \sigma_i\left(\text{erf}(\frac{\mu_i-a}{\sqrt{2}\sigma_i}) - \text{erf}(\frac{\mu_i-b}{\sqrt{2}\sigma_i})\right))
\]

For stability reasons we use the results of the leastsquares fit as
the start parameters of our minimum likelihood fit.  The minimizing
method is "L-BFGS-B"[CITE]. $\mu_i$ is the prediction of our model in
this case the result of the linear fit.  The number of charged
particles in the scintillators is poisson distributed. So the
estimator the energy has a standard deviance of $\sqrt{\mu}$.

Which is 
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from scipy.special import erf
  from scipy.stats import norm
  from scipy import optimize

  def likelihood(par):
      epsilon = 0.0000001
      mu = sum_n*par[0]+par[1]
      sigma = np.sqrt(np.abs(mu)) # 2/par[0] 
      elements = norm.pdf(energies, mu, sigma)
      a = np.divide(mu-lower_border, np.sqrt(2)*sigma+epsilon)
      b = np.divide(mu-upper_border, np.sqrt(2)*sigma+epsilon)
      norms = np.abs(erf(a)- erf(b))
      return -np.sum(np.log(np.divide(elements, norms + epsilon)+epsilon))

  lower_border = 0  
  upper_border = 10
  out = optimize.minimize(likelihood, np.array(c_fit), method='L-BFGS-B')
  
  c_like = out['x']

  n = np.arange(0, 240, 0.5)
  plt.plot(sum_n, energies, 'r.', alpha=0.06)
  plt.plot(n, fitfunc(c_like, n), 'k-')     # Fit

  plt.ylabel(r'$E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$\sum_i n_i$')

  plt.text(1, 9, r'$c_0 =$ '+ '%.3f' % c_like[0], ha='left')
  plt.text(1, 8.3, r'$c_1 =$ '+ '%.2f' % c_like[1], ha='left')

  plt.savefig('e-vs-sum_n_lik.pdf')
#+END_SRC

#+CAPTION: The graph is the same as in \cite{e-vs-sum_n}. The black straight is the result of the likelihood fit described above.
#+NAME: e-vs-sum_n_lik
[[./e-vs-sum_n_lik.pdf]] 

* First Network

We are starting with a fully connected neural network. It is build out of 4 layers with 64 neurons each. The activation function of each layer is ReLu.
Our output layer is one neuron with a linear activation. We train the network with the rmsprop optimizer and our loss function is the mean squared error between
the true energy values and our predicted energy values. We train the network for 10 epochs with a batch size of 128.

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from keras.models import Sequential
  from keras.layers import Dense

  model = Sequential()
  model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(128, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(64, activation='relu'))
  model.add(Dense(1, activation='linear'))

  model.compile(optimizer='rmsprop',
                loss='mse')

  loss_hist = np.empty(0)
  val_loss_hist = np.empty(0)
  epoch_arr = np.empty(0)
  hist = h5py.File('history.h5', 'w')
  hist.create_dataset('loss_hist', data=loss_hist)
  hist.create_dataset('val_loss_hist', data=val_loss_hist)
  hist.close()

  model.save('model.h5')
#+END_SRC

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import pickle
history = pickle.load(open('first_history.p', 'rb'))
epochs = range(len(history['loss']))
fig, ax = plt.subplots()
ax.spines['top'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
plt.tick_params(axis='both', which='both', bottom=False, top=False,
                labelbottom=True, left=True, right=False, labelleft=True)
ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))
plt.plot(epochs, history['loss'], 'k-')
plt.plot(epochs, history['val_loss'], '-', color='#1f77b4')
plt.text(float(epochs[-1])+0.5, history['loss'][-1], 'training loss', ha='left', va='center', size=15)
plt.text(float(epochs[-1])+0.5, history['val_loss'][-1], 'validation loss', ha='left', va='center', size=15, color='#1f77b4')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.savefig('loss.pdf', bbox_inches = 'tight')
#+END_SRC


#+CAPTION: The Graph shows the evolution of the loss function for the training set and the validation set.
#+NAME: Loss
[[./loss.pdf]]

In [cite:Loss] the loss function is shown. While the loss function for
the training set decreases over time, the loss for the validation set
increases. This implies that our model is overfitting, which means
that the model learns the data and nothing about the underlying
physics. Also 

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  model.load_weights('../source/weights.h5')
  fig, ax = plt.subplots()
  plt.plot(energies, fitfunc(c_like, sum_n) - energies, '.', alpha=0.25, markersize=3, color='#1f77b4')
  #plt.plot(Y_train[:10000], (model.predict(X_train[:10000])-Y_train[:10000]), '.', alpha=0.25, markersize=3, color='#1f77b4')
  plt.plot(Y_test[:10000], (model.predict(X_test[:10000])-Y_test[:10000]), 'k.', alpha=0.25, markersize=3)
  plt.plot(np.arange(11), np.ones(11), 'w-', linewidth=0.5)
  plt.ylim([-5., 5])
  plt.xlim([0.,10])
  plt.ylabel(r'$E_{\text{pred}} - E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)  
  ax.spines["left"].set_visible(False)
  ax.spines["bottom"].set_visible(False)  
#+END_SRC

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from ensure import ensure
  def measure_val(X, Y, n):
      '''
      Calculate mean and std of X following Y
      Y: numpy array with shape (,1)
      X: numpy array with the same shape as X
      n: len of each results (int and lower than len(Y))
      returns: value, mu, sigma
      '''
      ensure(len(Y.shape) == 2 and Y.shape[1] == 1).is_(True)
      ensure(Y.shape == X.shape).is_(True)
      ensure(n).is_an(int)
      ensure(n < len(Y)).is_(True)

      # find the sorted order of Y
      len_by_n = len(Y) - (len(Y) % n)
      perm = Y[:len_by_n, 0].argsort()
      # calculate results
      y = np.mean(np.transpose(Y)[0][perm].reshape(len_by_n/n, n), axis=1)
      mu = np.mean(np.transpose(X)[0][perm].reshape(len_by_n/n, n), axis=1)
      sigma = np.std(np.transpose(X)[0][perm].reshape(len_by_n/n, n), axis=1)
      return y, mu, sigma

  n = 1000
  y_f, mu_f, sigma_f = measure_val(fitfunc(c_like, np.sum(X_test, axis=1)).reshape(len(X_test), 1), Y_test, n)
  y_nn, mu_nn, sigma_nn = measure_val(model.predict(X_test), Y_test, n)
  y_ov, mu_ov, sigma_ov = measure_val(model.predict(X_train), Y_train, n*10)


  fig = plt.figure()
  ax = fig.add_subplot(2,1,1)

  ax.plot(y_f, mu_f - y_f, 'k.')
  ax.plot(y_nn, mu_nn - y_nn, 'r.')
  ax.plot(y_ov, mu_ov - y_ov, '.',color='#1f77b4')
  plt.ylabel(r'$\mu - E_{\text{true}}$ [GeV]')
  ax.xaxis.set_ticks([])
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)
  ax.spines["bottom"].set_visible(False)

  ax = fig.add_subplot(2,1,2)
  ax.plot(y_f, sigma_f/ np.sqrt(y_f), 'k.')
  ax.plot(y_nn, sigma_nn/np.sqrt(y_nn), 'r.')
  ax.plot(y_ov, sigma_ov/np.sqrt(y_ov), '.', color='#1f77b4')
  plt.ylabel(r'$\sigma / \sqrt{E_{\text{true}}}$')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)
#+END_SRC

** Server Stuff :noexport:
Here we copy stuff to the server and back
#+BEGIN_SRC shell
  scp model.h5 max-wgs:/home/schnakes/neural-net/
  scp max-wgs:/home/schnakes/neural-net/history.p .
  ssh max-wgs 'rm /home/schnakes/neural-net/history.p'
  ssh max-wgs 'rm /home/schnakes/neural-net/model.h5'
#+END_SRC

* Alternative Loss
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import tensorflow as tf

  def likelihood_loss(y_true, y_pred):
      epsilon = tf.constant(np.float64(0.0000001))
      lower_border = tf.constant(np.float64(0.))
      upper_border = tf.constant(np.float64(10.))
      two = tf.constant(np.float64(2.))
      pi = tf.constant(np.float64(np.pi))
      mu = y_pred
      sigma = tf.sqrt(tf.abs(mu))
      elements = tf.divide(tf.exp(tf.divide(- tf.square(mu - y_true),
                                            two*tf.square(sigma))),
                           tf.sqrt(two*pi)*sigma)
      a = tf.divide(mu-lower_border, tf.sqrt(two)*sigma+epsilon)
      b = tf.divide(mu-upper_border, tf.sqrt(two)*sigma+epsilon)
      norms = tf.abs(tf.erf(a) - tf.erf(b))
      return -tf.reduce_mean(tf.log(tf.divide(elements,
                                              norms + epsilon)
                                    + epsilon))

#+END_SRC
* Data Augmentation
One of the biggest issues with the setup is that it
leads to fast overfitting models. To compensate this, there are
different ways. One way is to artificially increase the dataset by
data augmentation. Data Augmentation means generating new data by transformation of the given data.
TODO <- BESSER ERKLÄREN.
An easy to understand example is shown in the image of the cat.  Examples
of data augmentation are every form of flipping, rotations or
cutting. In the perspective of a physicist, data augmentation could be
interpreted as a form of making the data invariant under symmetry
transformations. This is of course only a subset of the possible ways
of data augmentation, but it should be enough for our
application. Calorimeter events should be invariant under rotations
perpendicular to the direction of the incoming particle. Our image of
the physical process is processed in rectangular structures, which
makes only rotations around $\frac{\pi}{2}$ valid transformations.  To
reduce computation and data costs we randomly apply this
transformations on incoming data into our network. So we do not
tranform data we are not training on and we are not storing additional
transformed datasets.

** TODO Einbauen

* Are we learning the shape?
** TODO divide the incoming cells by the total sum

* Result
* Problem
* COMMENT Likelihood Solution
To diminish the depency of the distribution of given true labels, the
maximum likelihood fit, developed in the previous chapter, will be
used as a loss function.

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  from keras.layers import Dense, Input
  from keras.models import Model
  def likelihoodModel():
      X_input = Input(shape=(1088,))
      X = Dense(128, activation='relu', name='fc')(X_input)
      X = Dense(128, activation='relu', name='fc1')(X)
      X = Dense(128, activation='relu', name='fc2')(X)
      X = Dense(10, activation='relu', name='fc3')(X)
      X = Dense(1, activation='linear', name='exit')(X)
      return Model(inputs=X_input, outputs=X, name='likelihoodModel')

  model = likelihoodModel()
#+END_SRC


#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
import pickle
history = pickle.load(open('src/likelihood_history.p', 'rb'))
epochs = range(len(history['loss']))
fig, ax = plt.subplots()
ax.spines['top'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
plt.tick_params(axis='both', which='both', bottom=False, top=False,
                labelbottom=True, left=True, right=False, labelleft=True)
ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))
plt.plot(epochs, history['loss'], 'k-')
plt.plot(epochs, history['val_loss'], '-', color='#1f77b4')
plt.text(float(epochs[-1])+0.5, history['loss'][-1], 'training loss', ha='left', va='center', size=15)
plt.text(float(epochs[-1])+0.5, history['val_loss'][-1], 'validation loss', ha='left', va='center', size=15, color='#1f77b4')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.savefig('loss.pdf', bbox_inches = 'tight')
#+END_SRC

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  model.load_weights('src/likelihood_weights.h5')
  fig, ax = plt.subplots()
  plt.plot(energies, fitfunc(c_like, sum_n) - energies, '.', alpha=0.25, markersize=3, color='#1f77b4')
  # plt.plot(Y_train[:10000], (model.predict(X_train[:10000])-Y_train[:10000]), 'r.', alpha=0.25, markersize=3)
  plt.plot(Y_test[:10000], (model.predict(X_test[:10000])-Y_test[:10000]), 'k.', alpha=0.25, markersize=3)
  # plt.ylim([-6., 6])
  plt.xlim([0.,10])
  plt.ylabel(r'$E_{\text{pred}} - E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)  
  ax.spines["left"].set_visible(False)
  ax.spines["bottom"].set_visible(False)  
#+END_SRC


#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  def measure_val(X, Y, n):
      '''
      Calculate mean and std of X following Y
      Y: numpy array with shape (,1)
      X: numpy array with the same shape as X
      n: len of each results (int and lower than len(Y))
      returns: value, mu, sigma
      '''
      # ensure(len(Y.shape) == 2 and Y.shape[1] == 1).is_(True)
      # ensure(Y.shape == X.shape).is_(True)
      # ensure(n).is_an(int)
      # ensure(n < len(Y)).is_(True)

      # find the sorted order of Y
      len_by_n = len(Y) - (len(Y) % n)
      perm = Y[:len_by_n, 0].argsort()
      len_by = int(len_by_n/n)
      # calculate results
      y = np.mean(np.transpose(Y)[0][perm].reshape(len_by, n), axis=1)
      mu = np.mean(np.transpose(X)[0][perm].reshape(len_by, n), axis=1)
      sigma = np.std(np.transpose(X)[0][perm].reshape(len_by, n), axis=1)
      return y, mu, sigma

  n = 1000
  y_f, mu_f, sigma_f = measure_val(fitfunc(c_like, np.sum(X_test, axis=1)).reshape(len(X_test), 1), Y_test, n)
  y_nn, mu_nn, sigma_nn = measure_val(model.predict(X_test), Y_test, n)
  y_ov, mu_ov, sigma_ov = measure_val(model.predict(X_train), Y_train, n*10)


  fig = plt.figure()
  ax = fig.add_subplot(2,1,1)

  ax.plot(y_f, mu_f - y_f, 'k.')
  ax.plot(y_nn, mu_nn - y_nn, 'r.')
  ax.plot(y_ov, mu_ov - y_ov, '.',color='#1f77b4')
  plt.ylabel(r'$\mu - E_{\text{true}}$ [GeV]')
  ax.xaxis.set_ticks([])
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)
  ax.spines["bottom"].set_visible(False)

  ax = fig.add_subplot(2,1,2)
  ax.plot(y_f, sigma_f/ np.sqrt(y_f), 'k.')
  ax.plot(y_nn, sigma_nn/np.sqrt(y_nn), 'r.')
  ax.plot(y_ov, sigma_ov/np.sqrt(y_ov), '.', color='#1f77b4')
  plt.ylabel(r'$\sigma / \sqrt{E_{\text{true}}}$')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)
#+END_SRC

* Adverserial Solution
* Correction
