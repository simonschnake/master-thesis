* Setup                                                            :noexport:

Here are the needed packages. Also to config matplotlib for latex export
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import numpy as np 

  from scipy.optimize import leastsq
  import matplotlib as mpl
  import matplotlib.pyplot as plt
  mpl.rcParams['text.usetex'] = True
  mpl.rcParams['text.latex.preamble'] = [r'\usepackage{amsmath}']
  mpl.rcParams['mathtext.fontset'] = 'stix'
  mpl.rcParams['font.family'] = 'STIXGeneral'
  mpl.rcParams['font.size'] = 15
  mpl.rcParams['axes.labelsize'] = 15

  %matplotlib inline
  import pickle
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
:END:

* Loading Data                                                     :noexport:

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  dense_res = pickle.load(open('./results/dense_results.p', 'rb'))
  conv_res = pickle.load(open('./results/conv_results.p', 'rb'))
  #adv_res = pickle.load(open('./results/adv_results.p', 'rb'))
  # linear fit
  leng = len(dense_res['y_true']['raw'])
  sum_n = pickle.load(open('./results/sum_n.p', 'rb'))[:leng]

  inv_fitfunc = lambda c , x: (x-c[1])/c[0]

  fitfunc = lambda c , x: (x-c[1])/c[0]
  errfunc = lambda c , x, y: (y - fitfunc(c, x))
  out = leastsq(errfunc, [0.1, 0.0], args=(dense_res['y_true']['raw'], sum_n), full_output=1)

  c_fit = out[0]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

* Loss Raw

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
history = dense_res['history']
conv_history = conv_res['history']

epochs = range(len(history['loss']))
fig, ax = plt.subplots()
ax.spines['top'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
plt.tick_params(axis='both', which='both', bottom=False, top=False,
                labelbottom=True, left=True, right=False, labelleft=True)
ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))

plt.plot(epochs, history['loss'][:len(epochs)], 'k-')
plt.plot(epochs, history['val_loss'][:len(epochs)], '-', color='#BF616A')
# plt.plot(epochs, history['da_loss'], 'k-')
# plt.plot(epochs, history['da_val_loss'], '-', color='#1f77b4')

plt.text(float(epochs[-1])+0.5, history['loss'][-1], 'training loss', ha='left', va='center', size=16)
plt.text(float(epochs[-1])+0.5, history['val_loss'][-1], 'validation loss', ha='left', va='center', size=16, color='#BF616A')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.ylim([0.1, 1.0])
plt.savefig('images/dense_loss.pdf', bbox_inches = 'tight')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[45]:
[[file:./obipy-resources/KAkWPi.png]]
:END:

* Scatter Plot Raw
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export

  func = lambda c, x: c[0]*x+c[1] 
  fig, ax = plt.subplots()
  plt.plot(dense_res['y_true']['raw'][:10000], func(c_fit, sum_n)[:10000], '.', alpha=0.25, markersize=3, color='#1f77b4')
  plt.plot(dense_res['y_true']['raw'][:10000], dense_res['y_pred']['raw'][:10000], 'k.', alpha=0.25, markersize=3)
  # plt.ylim([-5., 5])
  # plt.xlim([0.,10])
  plt.ylabel(r'$E_{\text{pred}}$ [GeV]')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')

  plt.text(7, 3.5, 'neural net', ha='left', va='center', size=17)
  plt.text(7.5, 12, 'linear fit', ha='left', va='center', size=17, color='#1f77b4')

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)  
  ax.spines["left"].set_visible(False)
  ax.spines["bottom"].set_visible(False)
  # plt.savefig('images/dense_scatter.pdf', bbox_inches = 'tight')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[3]:
[[file:./obipy-resources/Rflkpc.png]]
:END:

* Results Raw

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  import sys
  sys.path.append('./src')
  from utils import *
  n = 20

  y_f, mu_f, sigma_f = sliced_statistics(dense_res['y_true']['raw'], func(c_fit, sum_n), n) 

  fig = plt.figure()
  ax = fig.add_subplot(2,1,1)

  # mu plot

  ax.plot(y_f, mu_f - y_f, '-', color='#1f77b4')
  ax.plot(dense_res['y']['raw'], dense_res['mu']['raw'] - dense_res['y']['raw'], 'k-')
  plt.text(y_f[-1] + 0.1, mu_f[-1] - y_f[-1]+0.02, 'linear fit', ha='left', va='center', size=15, color='#1f77b4')
  plt.text(dense_res['y']['raw'][-1] + 0.1, dense_res['mu']['raw'][-1] - dense_res['y']['raw'][-1]-0.02, 'neural net', ha='left', va='center', size=15)
  plt.ylabel(r'$\mu - E_{\text{true}}$ [GeV]')
  plt.ylim([-0.8, 0.8])
  ax.xaxis.set_ticks([])
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)
  ax.spines["bottom"].set_visible(False)


  # sigma/sqrt(y_true) plot

  ax = fig.add_subplot(2,1,2)
  ax.plot(y_f, sigma_f / np.sqrt(y_f), '-', color='#1f77b4')
  ax.plot(dense_res['y']['raw'], dense_res['sigma']['raw'] / np.sqrt(dense_res['y']['raw']), 'k-')
  plt.ylabel(r'$\sigma / \sqrt{E_{\text{true}}}$')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')
  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  plt.text(y_f[-1] + 0.1, sigma_f[-1] / np.sqrt(y_f[-1])+0.01, 'linear fit', ha='left', va='center', size=15, color='#1f77b4')
  plt.text(dense_res['y']['raw'][-1] + 0.1, dense_res['sigma']['raw'][-1] / np.sqrt(dense_res['y']['raw'][-1])-0.01, 'neural net', ha='left', va='center', size=15)
  plt.ylim([0., 0.5])
  # plt.savefig('images/dense_res.pdf', bbox_inches = 'tight')
#+END_SRC

* Loss Data Augmentation

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
history = dense_res['history']
conv_history = conv_res['history']
epochs = range(len(history['da_loss']))

fig, ax = plt.subplots()
ax.spines['top'].set_visible(False)
ax.spines['bottom'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.spines['left'].set_visible(False)
plt.tick_params(axis='both', which='both', bottom=False, top=False,
                labelbottom=True, left=True, right=False, labelleft=True)
ax.xaxis.set_major_formatter(plt.FuncFormatter('{:.0f}'.format))
plt.plot(epochs, history['loss'], '-', color='#433376')
plt.plot(epochs, history['val_loss'], '-', color='#8A7FB0')
plt.plot(epochs, history['da_loss'], '-', color='#A10D0D')
plt.plot(epochs, history['da_val_loss'], '-', color='#DD4A4A')

plt.text(float(epochs[-1])+0.5, history['loss'][-1]-0.02, 'neural net train', ha='left', va='center', size=15, color='#433376')
plt.text(float(epochs[-1])+0.5, history['val_loss'][-1]+0.02, 'neural net val', ha='left', va='center', size=15, color='#8A7FB0')
plt.text(float(epochs[-1])+0.5, history['da_loss'][-1]-0.02, 'data augment train', ha='left', va='center', size=15, color='#A10D0D')
plt.text(float(epochs[-1])+0.5, history['da_val_loss'][-1]+0.02, 'data augment val', ha='left', va='center', size=15, color='#DD4A4A')

plt.xlabel('epochs')
plt.ylabel('loss')
plt.ylim([0.1, 1.2])
plt.savefig('images/data_augment_loss.pdf', bbox_inches = 'tight')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[6]:
[[file:./obipy-resources/cr8DKs.png]]
:END:

* Scatter Likelihood

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export

  func = lambda c, x: c[0]*x+c[1] 
  fig, ax = plt.subplots()
  plt.plot(dense_res['y_true']['raw'][:10000], func(c_fit, sum_n)[:10000], '.', alpha=0.25, markersize=3, color='#1f77b4')
  plt.plot(dense_res['y_true']['likeli'][:10000], dense_res['y_pred']['likeli'][:10000], 'k.', alpha=0.25, markersize=3)
  # plt.ylim([-5., 5])
  # plt.xlim([0.,10])
  plt.ylabel(r'$E_{\text{pred}} - E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')

  plt.text(7, 3.5, 'neural net', ha='left', va='center', size=17)
  plt.text(7.5, 12, 'linear fit', ha='left', va='center', size=17, color='#1f77b4')

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)  
  ax.spines["left"].set_visible(False)
  ax.spines["bottom"].set_visible(False)  
#+END_SRC

* Comparison Loss Functions

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  fig, ax = plt.subplots()

  plt.plot(y_f, mu_f - y_f, '-', color='#BF616A', linewidth=3.0)
  plt.plot(dense_res['y']['da'], dense_res['mu']['da'] - dense_res['y']['raw'], '-', color='#D08770', linewidth=3.0)
  plt.plot(dense_res['y']['likeli'], dense_res['mu']['likeli'] - dense_res['y']['likeli'], '-', color='#3B4252', linewidth=3.0)

  plt.text(y_f[-1] + 0.1, mu_f[-1] - y_f[-1], 'linear fit', ha='left', va='center', size=15,  color='#BF616A', weight='bold')
  plt.text(dense_res['y']['da'][-1] + 0.1, dense_res['mu']['da'][-1] - dense_res['y']['raw'][-1], 'mse', ha='left', va='center', size=15,  color='#D08770', weight='bold')
  plt.text(dense_res['y']['likeli'][-1] + 0.1, dense_res['mu']['likeli'][-1] - dense_res['y']['likeli'][-1], 'likelihood', ha='left', va='center', size=15,  color='#3B4252', weight='bold')

  plt.ylabel(r'$\mu - E_{\text{true}}$ [GeV]')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')
  plt.ylim([-1., 0.2])

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)


  plt.savefig('images/loss_comparison.pdf', bbox_inches = 'tight')
#+END_SRC

* Comparison Fully Connected Net & Conv Net

#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export
  fig, ax = plt.subplots()

  plt.plot(y_f, sigma_f / np.sqrt(y_f), '-', color='#BF616A', linewidth=3.0)
  plt.plot(dense_res['y']['likeli'], dense_res['sigma']['likeli'] / np.sqrt(conv_res['y']['likeli']), '-', color='#D08770', linewidth=3.0)
  plt.plot(conv_res['y']['likeli'], conv_res['sigma']['likeli'] / np.sqrt(conv_res['y']['likeli']), '-', color='#3B4252', linewidth=3.0)

  plt.text(y_f[-1] + 0.1, sigma_f[-1]/np.sqrt(y_f[-1]), 'linear fit', ha='left', va='center', size=15,  color='#BF616A', weight='bold')
  plt.text(dense_res['y']['likeli'][-1] + 0.1, dense_res['sigma']['likeli'][-1]/np.sqrt(dense_res['y']['likeli'][-1]), 'dense', ha='left', va='center', size=15,  color='#D08770', weight='bold')
  plt.text(conv_res['y']['likeli'][-1] + 0.1, conv_res['sigma']['likeli'][-1]/np.sqrt(conv_res['y']['likeli'][-1]), 'conv', ha='left', va='center', size=15,  color='#3B4252', weight='bold')

  plt.ylabel(r'$\sigma/\sqrt{E_{\text{true}}}$')
  plt.xlabel(r'$E_{\text{true}}$ [GeV]')
  plt.ylim([0.25, 0.5])

  ax.spines["top"].set_visible(False)
  ax.spines["right"].set_visible(False)

  # plt.savefig('images/dense_res.pdf', bbox_inches = 'tight')
#+END_SRC

* Adversarial Scatter
#+BEGIN_SRC ipython :session :results raw drawer :exports none :eval no-export

  func = lambda c, x: c[0]*x+c[1] 

  fig = plt.figure(figsize=(20,10))

  for i in range(9):
      ax = fig.add_subplot(3,3,i+1)

      ax.plot(adv_res['y_true'][:10000], func(c_fit, sum_n)[:10000], '.', alpha=0.25, markersize=3, color='#1f77b4')
      ax.plot(adv_res['y_true'][:10000], adv_res['y_pred'][i][:10000], 'k.', alpha=0.25, markersize=3)
      plt.ylim([0., 12.])
      ax.xaxis.set_ticks([])
      ax.yaxis.set_ticks([])
      # plt.xlim([0.,10])
      #plt.ylabel(r'$E_{\text{pred}} - E_{\text{true}}$ [GeV]')
      #plt.xlabel(r'$E_{\text{true}}$ [GeV]')

      plt.text(2.5, 8, str(i+1), ha='left', va='center', size=20)
      #plt.text(7.5, 12, 'linear fit', ha='left', va='center', size=17, color='#1f77b4')

      ax.spines["top"].set_visible(False)
      ax.spines["right"].set_visible(False)  
      ax.spines["left"].set_visible(False)
      ax.spines["bottom"].set_visible(False)  
  plt.savefig('images/adv_scatter.pdf', bbox_inches = 'tight')
#+END_SRC



